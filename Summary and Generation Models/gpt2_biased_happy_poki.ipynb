{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gpt2_poem_happy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asMdbAgYCUZO"
      },
      "source": [
        "###Note that this jupiter notebook was built off of the notebook created by Scott Duda in this article:\n",
        "https://scottmduda.medium.com/generating-an-edgar-allen-poe-styled-poem-using-gpt-2-289801ded82c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPSUnmVIQj2x",
        "outputId": "d838a58e-6a3d-49b1-a55d-79ce3a3d0ce0"
      },
      "source": [
        "#Setup the environment\n",
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "home_directory = '/content/drive/MyDrive/Aps360 Project/MultiPoemDataset/emotions/happy'\n",
        "RANDOM_SEED = 73\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 8\n",
        "MAX_LEN = 1024"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqDVvA-3DADW"
      },
      "source": [
        "#This class stores the tokenized version of every poem and allows you to\n",
        "#Get a specific tokenized poem by its index in the list of poems.\n",
        "class PoemDataset(Dataset):\n",
        "    \n",
        "    #When initialied, tokenize each poem and store them in properties of the class\n",
        "    def __init__(self, data, tokenizer, gpt2_type='gpt2', max_length=MAX_LEN):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        \n",
        "        for i in data:\n",
        "            encodings_dict = tokenizer('<BOS>' + i + '<EOS>',\n",
        "                                     truncation=True,\n",
        "                                     max_length=max_length,\n",
        "                                     padding='max_length'\n",
        "                                    )\n",
        "\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    #Poems were tokenized and appended to the array in order,\n",
        "    #Allowing us to retrieve the tokens for the poems by the poem index\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaHaMTLpFWMG"
      },
      "source": [
        "#Helper functions\n",
        "def get_train_val_size(split, dataset):\n",
        "    train_size = int(split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return train_size, val_size\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJyFgORMNBIx"
      },
      "source": [
        "def train(poem_model,learning_rate=1e-4,eps=1e-8,warmup_steps=50,starting_epoch=0):\n",
        "  optimizer = AdamW(poem_model.parameters(), lr=learning_rate, eps=eps)\n",
        "  total_steps = len(poem_train_dataloader) * EPOCHS\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps=warmup_steps,\n",
        "                                              num_training_steps=total_steps)\n",
        "  start_time = time.time()\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  for epoch_i in range(starting_epoch, EPOCHS):\n",
        "      print(f'Epoch {epoch_i + 1} of {EPOCHS}')\n",
        "      t0 = time.time()\n",
        "\n",
        "      #Train the model\n",
        "      total_train_loss = 0\n",
        "      poem_model.train()\n",
        "      for step, batch in enumerate(poem_train_dataloader):\n",
        "\n",
        "          #Note that the labels are the same as the input. This is because the \n",
        "          #GPT2LMHeadModel That we are using shifts the labels by 1 meaning that \n",
        "          #the label for each input token is the next input token. This is desired \n",
        "          #when building a language model because we want the predicted output to\n",
        "          #be the next most likely word in the sentence. \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_labels = batch[0].to(device)\n",
        "          b_masks = batch[1].to(device)\n",
        "\n",
        "          poem_model.zero_grad()        \n",
        "          outputs = poem_model(b_input_ids,\n",
        "                                      labels=b_labels,\n",
        "                                      attention_mask=b_masks,\n",
        "                                      token_type_ids=None)\n",
        "          loss = outputs[0]  \n",
        "          batch_loss = loss.item()\n",
        "          total_train_loss += batch_loss\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "      avg_train_loss = total_train_loss / len(poem_train_dataloader)       \n",
        "      training_time = format_time(time.time() - t0)\n",
        "      print(f'Average Training Loss: {avg_train_loss}. Epoch Training Time: {training_time}')\n",
        "      \n",
        "      t0 = time.time()\n",
        "\n",
        "      #Evaluate the model\n",
        "      poem_model.eval()\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "      for batch in poem_val_dataloader:\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_labels = batch[0].to(device)\n",
        "          b_masks = batch[1].to(device)\n",
        "\n",
        "          with torch.no_grad():        \n",
        "              outputs  = poem_model(b_input_ids,\n",
        "                                          attention_mask=b_masks,\n",
        "                                          labels=b_labels)\n",
        "              loss = outputs[0]  \n",
        "\n",
        "          batch_loss = loss.item()\n",
        "          total_eval_loss += batch_loss        \n",
        "\n",
        "      avg_val_loss = total_eval_loss / len(poem_val_dataloader)\n",
        "\n",
        "      train_loss.append(avg_train_loss)\n",
        "      val_loss.append(avg_val_loss)\n",
        "      torch.save(poem_model.state_dict(), \"{}/{}_epoch_{}\".format(\n",
        "              home_directory,\"model_happy\",epoch_i))\n",
        "      print(f'Average Validation Loss: {avg_val_loss}')\n",
        "\n",
        "  np.savetxt(\"{}/{}_train_loss.csv\".format(home_directory, \"model_happy\"), train_loss)\n",
        "  np.savetxt(\"{}/{}_val_loss.csv\".format(home_directory, \"model_happy\"), val_loss)\n",
        "  print(f'Total Training Time: {format_time(time.time()-start_time)}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woZpz9b6MjtA"
      },
      "source": [
        "###Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqJJR8LEpZ_"
      },
      "source": [
        "#Load them poems we want to train our model with\n",
        "poem_df = pd.read_csv(home_directory + '/poki-happy-with-header.csv')\n",
        "poem_df = poem_df.fillna('')\n",
        "\n",
        "#Load the GPT2 tokenizer that will be used by PoemDataset to encode the poems. Add the \n",
        "#BOS,EOS and PAD tokens to the tokenized dictionary so that when we put these \n",
        "#Tokens around our poems to separate them, the tokenizer will know what to do with them.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "#create an object of the poemDataset class\n",
        "#that will hold an ordered list of the tokenized version of each poem\n",
        "poem_dataset = PoemDataset(poem_df['poems'].values, tokenizer, max_length=MAX_LEN)\n",
        "\n",
        "#Split the poem dataset into a training set and a validation set.\n",
        "poem_train_size, poem_val_size = get_train_val_size(split=0.8, dataset=poem_dataset)\n",
        "poem_train_dataset, poem_val_dataset = random_split(poem_dataset, [poem_train_size, poem_val_size])\n",
        "poem_train_dataloader = DataLoader(poem_train_dataset,\n",
        "                              sampler=RandomSampler(poem_train_dataset),\n",
        "                              batch_size=BATCH_SIZE)\n",
        "poem_val_dataloader = DataLoader(poem_val_dataset,\n",
        "                            sampler=SequentialSampler(poem_val_dataset),\n",
        "                            batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9BGYimjRFbt"
      },
      "source": [
        "##Setup the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93EInWoR8imp"
      },
      "source": [
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "\n",
        "#If you are only evaluating the model and don't want to train it again,\n",
        "#set training_desired to false. Otherwise set it to true and choose the \n",
        "#epoch you left off at last time to start Training from (0 if you haven't).\n",
        "load_previous_state_dict = True\n",
        "previous_state_dict_location = \"model_happy_epoch_7\"\n",
        "\n",
        "\n",
        "#Setup the pretrained GPT2 model\n",
        "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions=MAX_LEN).from_pretrained('gpt2', output_hidden_states=True)\n",
        "poem_model = GPT2LMHeadModel.from_pretrained('gpt2', config=configuration)\n",
        "poem_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "if load_previous_state_dict:\n",
        "  poem_model.load_state_dict(torch.load(\"{}/{}\".format(home_directory,previous_state_dict_location)))\n",
        "\n",
        "poem_model.cuda()\n",
        "poem_model = poem_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CofhHKByVVmi"
      },
      "source": [
        "##Train the model, if desired"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty0eNg-9Aj1s"
      },
      "source": [
        "training_desired = False\n",
        "\n",
        "#If training_desired is set to true, choose the epoch you left off at last time\n",
        "#to continue training from there (put it as 0 if you haven't yet)\n",
        "starting_epoch = 0\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 1e-4\n",
        "eps = 1e-8\n",
        "warmup_steps = 50\n",
        "if training_desired:\n",
        "  train(poem_model,learning_rate,eps,warmup_steps,starting_epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlyNDGEbVX5j"
      },
      "source": [
        "##Use the model to generate poems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtN2HoCSDAD8"
      },
      "source": [
        "# create text generation seed prompt\n",
        "prompt = \"<BOS> I love pizza\"\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns_xjc0JIsP2"
      },
      "source": [
        "previous_state_dict_location = \"model_happy_epoch_7\"\n",
        "poem_model.eval()\n",
        "sample_outputs = poem_model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length=MAX_LEN,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}